"""
Agente AI Principal
Integra LLM com ferramenta            # Comentar OpenAI temporariamente para usar Gemini
            # if openai_api_key:
            #     self.llm = ChatOpenAI(
            #         model="gpt-3.5-turbo",  # Modelo mais acess√≠vel
            #         temperature=0.7,
            #         max_tokens=1000,
            #         openai_api_key=openai_api_key
            #     )
            #     print("‚úÖ DEBUG - OpenAI GPT-3.5-turbo inicializado com sucesso")
            #     logger.info("‚úÖ LLM OpenAI inicializado")
            #     returna de mem√≥ria para conversas inteligentes
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List
import os
from datetime import datetime

# Imports para LLM e ferramentas
try:
    from langchain_openai import ChatOpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.schema import HumanMessage, SystemMessage
    from langchain.tools import Tool
    from langchain.agents import AgentExecutor, create_openai_functions_agent
    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.memory import ConversationBufferWindowMemory
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è Alguns imports do LangChain falharam: {e}")

logger = logging.getLogger(__name__)

class EthicCompanionAgent:
    """
    Agente AI principal que combina LLM com ferramentas e sistema de mem√≥ria
    """
    
    def __init__(self):
        """Inicializa o agente com LLM e ferramentas"""
        self.llm = None
        self.agent_executor = None
        self.tools = []
        self._initialize_llm()
        self._initialize_tools()
        self._initialize_agent()
    
    def _initialize_llm(self):
        """Inicializa o modelo de linguagem (Google Gemini - OpenAI temporariamente desativado)"""
        try:
            # Debug: Verificar chaves API
            openai_api_key = os.getenv("OPENAI_API_KEY")
            google_api_key = os.getenv("GOOGLE_API_KEY")
            
            print(f"üîç DEBUG - OpenAI API Key: {'‚úÖ Carregada' if openai_api_key else '‚ùå N√£o encontrada'}")
            print(f"üîç DEBUG - Google API Key: {'‚úÖ Carregada' if google_api_key else '‚ùå N√£o encontrada'}")
            
            # Comentar OpenAI temporariamente para usar Gemini
            # if openai_api_key:
            #     self.llm = ChatOpenAI(
            #         model="gpt-3.5-turbo",  # Modelo mais acess√≠vel
            #         temperature=0.7,
            #         max_tokens=1000,
            #         openai_api_key=openai_api_key
            #     )
            #     print("‚úÖ DEBUG - OpenAI GPT-3.5-turbo inicializado com sucesso")
            #     logger.info("‚úÖ LLM OpenAI inicializado")
            #     return
                
        except Exception as e:
            print(f"‚ö†Ô∏è DEBUG - OpenAI desativado temporariamente: {e}")
            logger.warning(f"‚ö†Ô∏è OpenAI desativado temporariamente: {e}")
        
        try:
            # Usar Google Gemini como principal
            if google_api_key:
                self.llm = ChatGoogleGenerativeAI(
                    model="gemini-1.5-flash",  # Modelo mais recente dispon√≠vel
                    temperature=0.7,
                    google_api_key=google_api_key
                )
                print("‚úÖ DEBUG - Google Gemini 1.5 Flash inicializado como LLM principal")
                logger.info("‚úÖ LLM Google Gemini inicializado")
                return
                
        except Exception as e:
            print(f"‚ùå DEBUG - Falha ao inicializar Gemini: {e}")
            logger.warning(f"‚ùå Falha ao inicializar Gemini: {e}")
        
        # Se nenhum LLM foi inicializado
        print("‚ùå DEBUG - NENHUM LLM DISPON√çVEL - USANDO FALLBACK")
        logger.error("‚ùå Nenhum LLM dispon√≠vel")
        self.llm = None
    
    def _initialize_tools(self):
        """Inicializa ferramentas dispon√≠veis para o agente"""
        
        # Ferramenta de pesquisa web (Tavily)
        if os.getenv("TAVILY_API_KEY"):
            try:
                web_search_tool = Tool(
                    name="web_search",
                    description="Pesquisa informa√ß√µes atuais na web. Usa quando precisas de informa√ß√µes recentes ou espec√≠ficas que n√£o tens.",
                    func=self._web_search
                )
                self.tools.append(web_search_tool)
                logger.info("‚úÖ Ferramenta de pesquisa web ativada")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Falha ao configurar pesquisa web: {e}")
        
        # Ferramenta de an√°lise √©tica
        ethical_analysis_tool = Tool(
            name="ethical_analysis",
            description="Analisa quest√µes √©ticas complexas usando princ√≠pios de filosofia moral e √©tica aplicada.",
            func=self._ethical_analysis
        )
        self.tools.append(ethical_analysis_tool)
        
        # Ferramenta de reflex√£o pessoal
        personal_reflection_tool = Tool(
            name="personal_reflection",
            description="Facilita reflex√£o pessoal e autoconhecimento atrav√©s de perguntas guiadas.",
            func=self._personal_reflection
        )
        self.tools.append(personal_reflection_tool)
        
        logger.info(f"‚úÖ {len(self.tools)} ferramentas inicializadas")
    
    def _initialize_agent(self):
        """Inicializa o agente executivo com LLM e ferramentas"""
        if not self.llm:
            logger.error("‚ùå N√£o √© poss√≠vel criar agente sem LLM")
            return
        
        try:
            # Prompt do sistema para o agente
            system_prompt = """√âs o Ethic Companion, um assistente de IA especializado em √©tica, filosofia e desenvolvimento pessoal.

A tua miss√£o √© ajudar as pessoas a:
1. ü§î Refletir sobre quest√µes √©ticas complexas
2. üå± Desenvolver um pensamento cr√≠tico mais profundo  
3. üß≠ Navegar dilemas morais com sabedoria
4. üí≠ Facilitar autoconhecimento e crescimento pessoal

Caracter√≠sticas da tua personalidade:
- Emp√°tico e compreensivo
- Questionador sem ser julgmental
- S√°bio mas humilde
- Pr√°tico nas sugest√µes
- Respeitoso com todas as perspetivas

Usa as ferramentas dispon√≠veis quando apropriado:
- web_search: Para informa√ß√µes atuais ou espec√≠ficas
- ethical_analysis: Para quest√µes √©ticas complexas
- personal_reflection: Para facilitar autoconhecimento

Responde sempre em portugu√™s e mant√©m um tom caloroso e acess√≠vel."""

            # Tentar criar agente com ferramentas (se dispon√≠veis)
            if hasattr(self, 'tools') and self.tools:
                try:
                    # Template do prompt compat√≠vel com Gemini
                    prompt = ChatPromptTemplate.from_messages([
                        ("system", system_prompt),
                        MessagesPlaceholder(variable_name="chat_history"),
                        ("human", "{input}"),
                        MessagesPlaceholder(variable_name="agent_scratchpad")
                    ])
                    
                    # Tentar criar agente OpenAI functions (pode funcionar com Gemini)
                    agent = create_openai_functions_agent(self.llm, self.tools, prompt)
                    self.agent_executor = AgentExecutor(
                        agent=agent,
                        tools=self.tools,
                        verbose=True,
                        handle_parsing_errors=True,
                        max_iterations=3,
                        return_intermediate_steps=True
                    )
                    logger.info("‚úÖ Agente com ferramentas inicializado")
                    print("‚úÖ DEBUG - Agente com ferramentas criado com sucesso")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è DEBUG - Agente com ferramentas falhou, usando LLM direto: {e}")
                    logger.warning(f"‚ö†Ô∏è Falha ao criar agente com ferramentas: {e}")
                    # Fallback para LLM simples
                    self.agent_executor = None
            else:
                self.agent_executor = None
                logger.info("‚úÖ LLM simples inicializado (sem ferramentas)")
                print("‚úÖ DEBUG - LLM simples configurado")
                
        except Exception as e:
            print(f"‚ùå DEBUG - Erro cr√≠tico ao inicializar agente: {e}")
            logger.error(f"‚ùå Erro ao inicializar agente: {e}")
            self.agent_executor = None
    
    async def process_message(self, message: str, session_id: str) -> Dict[str, Any]:
        """
        Processa uma mensagem do utilizador e retorna resposta
        
        Args:
            message: Mensagem do utilizador
            session_id: ID da sess√£o para contexto
            
        Returns:
            Dict com resposta e metadados
        """
        try:
            start_time = datetime.now()
            
            # Se temos agente com ferramentas
            if self.agent_executor:
                try:
                    result = await asyncio.get_event_loop().run_in_executor(
                        None, 
                        lambda: self.agent_executor.invoke({
                            "input": message,
                            "chat_history": []  # Mem√≥ria gerida externamente
                        })
                    )
                    
                    response_text = result.get("output", "Desculpa, n√£o consegui processar a tua mensagem.")
                    tools_used = self._extract_tools_used(result)
                    
                except Exception as e:
                    print(f"üö® DEBUG - ERRO NO AGENTE EXECUTOR: {e}")
                    print(f"üö® DEBUG - TIPO DE ERRO: {type(e).__name__}")
                    logger.error(f"‚ùå Erro no agente executor: {e}")
                    # Fallback para LLM direto
                    response_text = await self._direct_llm_response(message)
                    tools_used = []
            
            # Se s√≥ temos LLM direto
            elif self.llm:
                response_text = await self._direct_llm_response(message)
                tools_used = []
            
            # Se n√£o temos nenhum LLM
            else:
                response_text = self._fallback_response(message)
                tools_used = []
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "response": response_text,
                "session_id": session_id,
                "processing_time": processing_time,
                "tools_used": tools_used,
                "llm_type": self._get_llm_type(),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico no processamento: {e}")
            return {
                "response": "Pe√ßo desculpa, ocorreu um erro inesperado. Podes tentar reformular a tua pergunta?",
                "session_id": session_id,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def _direct_llm_response(self, message: str) -> str:
        """Resposta direta do LLM sem ferramentas"""
        try:
            system_msg = SystemMessage(content="""√âs o Ethic Companion, especializado em √©tica e desenvolvimento pessoal. 
Responde de forma emp√°tica, reflexiva e em portugu√™s.""")
            
            human_msg = HumanMessage(content=message)
            
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.llm.invoke([system_msg, human_msg])
            )
            
            return response.content
            
        except Exception as e:
            logger.error(f"‚ùå Erro na resposta direta do LLM: {e}")
            return "Desculpa, tive dificuldades em processar a tua mensagem. Podes tentar novamente?"
    
    def _fallback_response(self, message: str) -> str:
        """Resposta de fallback quando nenhum LLM est√° dispon√≠vel"""
        return """Ol√°! Sou o Ethic Companion e estou aqui para ajudar com quest√µes √©ticas e reflex√µes pessoais.

Atualmente estou em modo limitado, mas posso oferecer algumas sugest√µes b√°sicas:

ü§î **Para dilemas √©ticos**: Considera os princ√≠pios de autonomia, benefic√™ncia, n√£o-malefic√™ncia e justi√ßa.

üí≠ **Para reflex√£o pessoal**: Pergunta-te: "Quais s√£o os meus valores fundamentais?" e "Como posso alinhar as minhas a√ß√µes com esses valores?"

üå± **Para crescimento**: Pequenos passos consistentes s√£o mais eficazes que grandes mudan√ßas espor√°dicas.

Podes reformular a tua pergunta ou aguardar que o sistema seja totalmente restaurado."""
    
    def _extract_tools_used(self, agent_result: Dict) -> List[str]:
        """Extrai ferramentas usadas do resultado do agente"""
        tools_used = []
        
        # Esta implementa√ß√£o depende da estrutura espec√≠fica do LangChain
        # Pode precisar de ajustes baseados na vers√£o
        if "intermediate_steps" in agent_result:
            for step in agent_result["intermediate_steps"]:
                if hasattr(step, 'tool') and step.tool:
                    tools_used.append(step.tool)
        
        return tools_used
    
    def _get_llm_type(self) -> str:
        """Identifica o tipo de LLM em uso"""
        if not self.llm:
            return "none"
        
        if "openai" in str(type(self.llm)).lower():
            return "openai"
        elif "google" in str(type(self.llm)).lower():
            return "gemini"
        else:
            return "unknown"
    
    # IMPLEMENTA√á√ïES DAS FERRAMENTAS
    
    def _web_search(self, query: str) -> str:
        """Ferramenta de pesquisa web usando Tavily"""
        try:
            # Implementa√ß√£o simplificada - expandir conforme necess√°rio
            return f"Resultados de pesquisa para '{query}': [Implementar integra√ß√£o Tavily]"
        except Exception as e:
            return f"Erro na pesquisa web: {e}"
    
    def _ethical_analysis(self, dilemma: str) -> str:
        """Ferramenta de an√°lise √©tica estruturada"""
        try:
            analysis = f"""
**An√°lise √âtica do Dilema:**

**Contexto:** {dilemma}

**Perspetivas √âticas:**
üèõÔ∏è **Deontol√≥gica (Kant):** Foca no dever e nas regras universais
‚öñÔ∏è **Consequencialista (Utilitarismo):** Avalia resultados e bem-estar geral  
üåü **Virtudes (Arist√≥teles):** Considera o car√°ter e virtudes morais
ü§ù **√âtica do Cuidado:** Enfatiza rela√ß√µes e responsabilidade

**Quest√µes para Reflex√£o:**
- Quais s√£o os stakeholders envolvidos?
- Que valores est√£o em conflito?
- Quais as consequ√™ncias de cada op√ß√£o?
- Que princ√≠pios √©ticos se aplicam?

**Pr√≥ximos Passos:** Considera estas perspetivas e identifica a tua intui√ß√£o moral inicial.
"""
            return analysis
        except Exception as e:
            return f"Erro na an√°lise √©tica: {e}"
    
    def _personal_reflection(self, topic: str) -> str:
        """Ferramenta para facilitar reflex√£o pessoal"""
        try:
            reflection = f"""
**Reflex√£o Pessoal sobre: {topic}**

**Perguntas Orientadoras:**
üí≠ O que sentes quando pensas neste tema?
üéØ Quais s√£o os teus valores relacionados com isto?
üîç Que experi√™ncias passadas influenciam a tua perspetiva?
üå± Como gostarias de crescer nesta √°rea?
‚≠ê Que pequeno passo podes dar hoje?

**Sugest√£o:** Dedica alguns minutos a escrever as tuas respostas. A escrita ajuda a clarificar pensamentos.

**Lembra-te:** N√£o h√° respostas certas ou erradas - apenas a tua verdade pessoal neste momento.
"""
            return reflection
        except Exception as e:
            return f"Erro na reflex√£o pessoal: {e}"
    
    def get_agent_status(self) -> Dict[str, Any]:
        """Retorna status do agente para diagn√≥stico"""
        return {
            "llm_available": self.llm is not None,
            "llm_type": self._get_llm_type(),
            "agent_executor_available": self.agent_executor is not None,
            "tools_count": len(self.tools),
            "tools_available": [tool.name for tool in self.tools],
            "status": "operational" if self.llm else "limited"
        }

# Inst√¢ncia global do agente
_ai_agent: Optional[EthicCompanionAgent] = None

def get_ai_agent() -> EthicCompanionAgent:
    """
    Depend√™ncia FastAPI para obter agente AI
    
    Returns:
        EthicCompanionAgent: Inst√¢ncia do agente configurado
    """
    global _ai_agent
    
    if _ai_agent is None:
        _ai_agent = EthicCompanionAgent()
    
    return _ai_agent

def reinitialize_agent():
    """Reinicializa o agente (√∫til para atualiza√ß√µes de configura√ß√£o)"""
    global _ai_agent
    _ai_agent = None
    return get_ai_agent()
